\chapter{结论与展望}
\section{本文主要工作总结}
移动机器人在人类的生产生活中扮演着重要角色，增量式定位是移动机器人在陌生环境中实现自主移动、地图构建和环境感知需要解决的重要问题，本文主要研究了增量式定位算法中的单目视觉里程计方法，重点解决单目视觉里程计的绝对尺度计算问题和端到端映射学习问题，并完成了以下工作：
\begin{enumerate}
    \item 提出了一种基于路面几何模型的单目视觉里程计尺度计算方法。我们首次提出使用路面几何结构进行路面特征点筛选，筛选依据为路面上特征点的深度一致性和法向一致性这两个必要条件。在具体实现上，我们使用三角剖分算法将路面特征点分割成三角形区域，并对每个三角形的三个顶点验证法向一致性，同时对每条边验证深度一致性，此外我们使用图模型来构建相邻三角形的相互影响以提高筛选准确性。最后使用筛选得到的路面特征点通过随机抽样一致方法计算路面模型，并进行尺度参数计算。

    \item 提出了一种结合单目深度估计的视觉里程计尺度计算方法。首先通过构建条件随机场模型，使用帧内临近像素点的深度连续约束和已知的机器人运动情况下相邻帧图像的光度一致约束提高深度估计精度，同时使用所估计的深度与单目视觉里程计根据运动视差和三角测量所计算获取相对深度进行统计分析，并使用学生t分布建模以获取尺度系数。将深度估计和尺度计算融合，通过迭代优化进行精度的提升。
    
    \item 提出了一种基于映射同态性的单目视觉里程计网络损失函数设计方法。我们分析视觉里程计的输入图像对和输出运动之间的映射关系，通过定义图像对集合的二元运算构建图像对广义群，并依据图像对群和运动所在李群之间的同态性设计了可以更好表征运动模型的损失函数，主要包括单位元损失函数、逆元损失函数和封闭性损失函数。并通过实验证明了，新定义的损失函数可以提升运动估计的准确性。

    \item 提出了一种基于区域一致性的单目视觉里程计网络架构设计方法。我们首次提出将图像区域作为基本估计单元，首先通过每个图像子区域计算相机运动，最后再融合得到最终相机运动。这种方法可以降低运动估计时对全局图像色彩分布的依赖，在一定程度上提高了算法在不同数据集上的泛化性。我们同时预测图像子区域所估计运动的可靠性差异，用以避免对无特征区域的过度拟合，同时我们利用区域运动可靠性计算相机运动可靠性，为失效判断和多传感器融合提供基础。
    
    \item 提出了一种针对路面车辆的视觉里程计映射学习方法。我们根据路面车辆的特点，简化了单目视觉里程计问题，只学习车辆主要维度运动，同时研究旋转运动与平移运动之间的耦合关系，并利用此关系降低忽略次要运动时造成的运动偏移。此外，我们根据车辆运动时光流的分布提出使用非正方形的卷积核进行特征提取。最终，我们设计了一个十分轻量的视觉里程计运动估计模型，可以每秒200帧的速度实时运行在CPU上。
\end{enumerate}

本文提出的所有算法均在公开数据集上进行了充分的测试和验证，同时我们开源了本文算法中所涉及到的大部分代码。

\section{进一步研究方向}
本文工作在一定程度上推进了单目视觉里程计的尺度计算和映射学习问题，但依然存在着一定的局限性，存在进一步研究的意义，以下四个方面为可以进一步研究的方向：
\begin{enumerate}
    \item 研究泛化性更强的尺度计算方法。本文所提出的基于路面几何模型和场景深度估计的尺度计算方法均受到场景限制，虽然基于场景深度估计的方法摆脱了路面平整和相机高度假设，但其依然依赖于测试场景和训练集场景的相似性。可研究场景中绝对尺度参考的自主寻找、使用和更新方法，在具体实现上，可以按照如下思路：首先构建绝对尺度参考库，将场景中常见的物体尺度记录到数据库中；在机器人运行过程中实时对场景进行实例分割，在场景中自主地寻找相对稳定的绝对参考，并用以抑制尺度漂移；同时根据尺度的一致性，对数据库中的绝对尺度参考进行更新和修正。
    \item 研究纯虚拟数据集训练。这个研究可包含两方面工作：首先可不考虑数据的真实性，生成完全随机的RGB图像、深度图像以及随机的相机运动，用以训练构建的模型。由于完全随机的数据无穷无尽，所以可以不间断地产生新的数据一直训练网络，进而得到一个泛化性更强的网络，同时可结合迁移学习来提高算法在真实数据的性能；另外可研究生成仿真数据，即在生成数据时考虑其与现实场景的可区分性，使用生成对抗网络来生成更接近真实数据的虚拟数据用以训练网络。
    \item 研究动态场景下单目视觉状态估计。静态环境假设是视觉里程计一个比较重要的假设，目前大部分方法均没有对动态环境下的视觉里程计问题进行系统化的研究，较为主流的解决方案依然是动态物体剔除。本文提出一种潜在的研究思路：首先按照场景中物体的运动模式将物体运动模态分为静态、模式化运动和随机性运动，然后对三种运动模式采用不同的方案处理：保留静态物体，建模分析模式化运动物体，直接剔除随机性运动物体。在具体实现上，可根据单目图像上物体的识别和跟踪以及机器人自身运动的初始估计判断场景中物体的运动模式；然后依据物体运动模式先验信息以及物体运动的实时观测对模式化运动物体的运动建模，并联合估计自身运动与场景中物体运动。
    \item 研究光照恶劣环境下的单目视觉里程计问题。光照恶劣环境下视觉里程计效果的局限性存在三方面原因：首先照度低的环境会使图像信噪比较低，进而影响到特征提取和匹配的稳定性；其次光照空间不均匀致使的图像亮度不均会导致提取特征无法在图像上均匀地进行；最后光照时间维度上的变化会导致特征匹配或光度误差计算出现偏差，影响状态估计精度。现有解决方案一般为两段式方法：首先将图像变换到适合进行状态估计的图像空间或者特征空间，然后在新的空间进行位姿估计。在这种方案中，新的特征空间一般为认为设定的，具有局限性。可研究图像变换与单目状态估计深度融合，在前期所研究的端到端状态估计方法基础之上，将图像变换网络与状态估计的网络串联，将状态估计的误差反向传播给图像变换网络，用以自主寻找更适合做状态估计的图像空间或特征空间。
\end{enumerate}
