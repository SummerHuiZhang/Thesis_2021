\section{MVOSR}
\subsubsection{自身状态估计的重要性}
自身状态估计是其中一个十分重要的问题，主要包括机器人
对自身所在的位置和姿态的估计，因为在机器人按照既定路线从起点移动到终点的过程中，需要持续对自身的位置和姿态进行估计并将所得到的
信息作为先验用于环境感知，地图构建，轨迹跟踪等问题。总而言之，自身状态估计是移动机器人需要具备的、不可或缺的核心技能。

自身状态估计主要解决机器人
对自身所在位置和姿态的估计，所得到的信息，因为其提供因为在机器人按照既定路线从起点移动到终点的过程中，需要持续对自身的位置和姿态进行估计并将所得到的
信息作为先验用于环境感知，地图构建，轨迹跟踪等问题。总而言之，自身状态估计是移动机器人需要具备的、不可或缺的核心技能。

1. 作为是否走丢的反馈或依据
2. 判断应该继续怎么走
3. 地图构建的基础
4. 辅助环境感知

\section{HomoVO}

单目视觉里程计问题是研究以单目图像序列作为输入信息的机器人状态估计问题，其依据机器人相邻时刻所捕获图像的变化计算机器人运动，进而增量式获取移动机器人的运动轨迹。
这个问题在2014年由Nister等人\cite{nister2004visual}首次正式定义, 但相关的工作可以追溯到大约40年前 \cite{fraundorfer2011visual}的火星探索工作。
\subsubsection{视觉里程计问题的特殊性}
举例来说：对于一个分
割问题，我可以使用 0 表示道路， 1 表示建筑， 2 表示车辆，但 0 与 1 和建筑与
交通并没有任何关系，道路 (0) 与建筑 (1) 的差异也不会比道路（0）与车辆（2）
的差距小。 （表述不清）

均值表示 \mu

损失函数 正常的L

\section{PADVO}
\paragraph{真值如何表示} 下划线
\begin{equation}
    \mathbf{L} = \sum_t \sum_p {F_{re}\left(^p\mathbf{I}_t^{t+1}\right)}\|F_{pa}\left(^p\mathbf{I}_t^{t+1}\right)-_g\mathbf{T}_t^{t+1}\|_2
\end{equation}
\paragraph{括号内内容}
Global Average Pooling 
首字母需要大写吗？全文都检查一下 （查过了）

全文统一
（记为 KITTI-00）

既然有了名称“表 5.4”这里不适合用表1 
可以加波折号另示区别 
表5.5同理

表格 title问题

图被遮挡

上述方法直接将整幅图像所在的高维空间集合做为映射函数的定义域。假设输入图像的尺寸为$w\times h$，通道数为$c$，那么输入图像对$\mathbf{I}_m^n \in \mathbb{I}^{w\times h\times 2c}$，其中$\mathbb{I}$为0到255之间所有整数的集合。图像尺寸越大，$\mathbb{I}^{w\times h\times 2c}$集合所在的空间维度越高，而集合中全集的元素数量与集合维度呈指数映射关系，所以，当图像尺寸较大时，需要更多次更均匀的训练样本才能较好的拟合集合全集。然而，由于现有数据集样本数量的不足以及场景内抽样样本的各个维度的相互耦合，样本空间一般仅分布在集合全集中的一小部分，使不同数据集的分布相差较大，学习到的映射模型无法很好的直接迁移。综上，我们认为全集空间的过大以及其间接导致的不完全采样是视觉里程计映射学习泛化性较差的一个重要原因。

上述方法直接将整幅图像做为映射函数的定义域，通过神经网络将整幅图像映射到相机运动，
这种朴素的解决方案会使学习到的网络模型过于依赖
这种朴素的解决方案引入了一个问题：这样学习到的神经网络模型过于
依赖场景色彩信息，而不同场景中图像整体分布往往差异很大。一幅宽为$w$高为$h$的图像$I$可以表示为$w\times h$个变量的联合概率分布，而这些变量往往不是相互独立的，不同的场景一般服从着不同的概率分布，这些概率分布由于过于复杂，不同场景的分布差异一般较大，我们认为这是当前端到端的视觉运动估计泛化性较差的主要原因之一。而数据量相同的情况下，输入图像维度越小，数据集所涵盖的分布就越广，测试集满足训练集数据分布的概率就越高，网络模型的泛化能力也就越强。



并尝试应用
这一性质提升端到端的单目视觉里程计的场景泛化能力，因为在不同场景中，图像子区域相似性会高于整幅图片的相似性，所以子区域空间上进行运动估计，从理论上可以将网络的定义域分布变广，进而避免对场景的过度依赖

实验描述

\section{datavo}
图中注释统一

全文结果汇总