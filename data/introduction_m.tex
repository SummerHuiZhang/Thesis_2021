\chapter{引言}
\label{ch:introduction}
\section{机器人定位研究背景和意义}

本论文以移动机器人在已知和未知环境中基于单目相机的自我定位为研究背景，主要研究绝对式定位的深度特征提取与压缩、增量式定位之单目视觉里程计（VO）的绝对尺度运算及VO问题的求解框架简化。本章节将从国际人工智能发展背景、国内机器人行业发展需求、移动机器人的发展需求及研究热点等多个角度逐层剖析。

国内背景：中国制造2025\cite{2017Made}，从第四代工业革命介绍人工智能的发展潮流，其中机器人尤其是无人驾驶的地位，简述机器人的研究难点，VO在其中的重要性，VO的难点，VO的关键技术
%定位的重要性与必要性： %早期人类如何定位的，对于人们的日常，以及航海、外太空探索等重要的研究项目，定位的重要性：
人工智能技术的出现，为人们生活带来翻天覆地的转变，机器人正在以多种存在形式逐渐融入人类的日常生活，大大提高了人类的生活质量与幸福感。人工智能技术的发展历经曲折却又在一代又一代人的努力下，不断创新与颠覆。1956年达特茅斯会议，标志着AI的诞生，第一个神经网络由Rosenblatt在1957年提出；但是由于计算能力的限制，没能使机器完成大规模数据训练和复杂任务，AI进入第一个低谷时期；1986年，BP算法的出现使得大规模神经网络的训练成为可能，带来AI的第二个黄金时代；但是，由于AI计算机DARPA没能实现，政府支持度降低，资金投入缩减，使AI进入了第二个低谷；2006年，Hilton提出“深度学习神经网络”，使AI出现了突破性转折，并先后在机器视觉识别、智能语音等领域取得成功，宣告了AI的爆发式发展，表明人类正式进入智能感知时代，带入第四代工业革命。机器人正以不同的身份出现在人们生活的方方面面，逐渐更新我们的生活方式。机器人出现在在林林总总的服务行业中，改变了传统产品形态与服务模式，比如送餐机器人、青少儿教育机器人、老人陪护与家庭助理型机器人、虚拟/增强现实等交互手段等。
同时，智能机器人具有人力劳动无法比拟的优点，比如精度更高、安全性更高、连续工作，既高效完成任务又减少了人力劳动的成本与危险。智能机器人在工业、农业、交通等方面具有绝佳的研究价值与商业价值。比如智能机器人能够完成工业行业的高强度与高难度的各种作业——核电作业、能源矿山作业、矿山勘探开采、水下考察等任务；%湖南大学研发智能桥梁检测特种机器人
在农业方面常见的有播种机器人、收割机器人等；在安全监测、智慧巡检、特殊运输等方面也出现了很多研究热点，例如深圳一清科技研发的“夸父-I”无人车在重灾区、一线城市郊野和产业园区等场所往返运输生活用品、医疗器械等；
在医疗保健行业也存在广大的应用场景，比如打造智慧医疗、疫情防控、救援抢险、助老助残等智能体，本质上解决医生的长时间工作疲劳问题，同时提高医生工作操作精准度，降低患者痛苦加速伤口愈合。外骨骼机器人——模仿人体骨骼结构设计的一种机电一体化装置，通过感知穿戴者的意图，
帮助残障人士更自主的生活，同时帮助负重等工作人士的身体损伤，提升人体机能。%又如在2020年初新型冠状病毒肺炎大肆感染的同时，湖南大学自主研制了测温-诊断智能机器人、智能消毒机器人、
医药物资搬运机器人等，投入多家医院使用。在军事防御、外太空空间探索、极地科考、深海遨游%、应急安防——在外太空、极地以及深海
等极限环境中降低对人类对挑战，拓展人类能力，利用多模态的感知手段为复杂地形下的自主探测提供了新的解决方案，例如2016年10月26日，美国军方
用3架F/A-18“超级大黄蜂”战斗机搭载并释放了103架“山鹑”（Perdix）微型无人机组成的机群，进行了无人机集群飞行试验。2017年以色列在第二届国际陆战会议上展示了“卡梅尔”自助驾驶装甲车。

智能机器人的发展趋势呈现自主化、协作化、灵巧化。
%    自主化：感知、处理、决策、执行等模块，机器人像人类一样能够灵活的处理出现的一些状况，甚至替人类完成高难度的任务，使人工智能作为人类和智能世界的桥梁。
%    协作化：
%    灵巧化：
%\subsection{研究挑战}
%2019年，《科学：机器人》杂志发表了关于智能机器人目前的十大挑战，包括：新材料和制造方案、仿生机器人、动力和能源、机器人集群、导航和探索、智能机器人、社会交互、脑机接口、医疗机器人、机器人伦理及安全。
    
从以上不同应用领域我们不难发现，移动机器人是第四代工业革命中不可或缺的核心技术，是保证人工智能服务于人类的最常见的机器人外在形态之一。%、多足机器人、人形机器人、手机/电脑、服务机器人、手术机器人、空中机器人、AR/VR机器人
移动机器人的代表之一就是无人驾驶车辆，其是指在没有人工干预的情况下，能够感知环境并进行路径规划、导航避障自主到达目的地的汽车。被科学家普遍认可的无人驾驶汽车出现在1921年，
是美国军队在俄亥俄州空军基地展示的一种三轮拖车，因为这辆拖车上配备了无线电控制系统，所以拖车上无需坐人。通用汽车在1956年的时候，推出的一辆Firebird II概念车，成为了无人驾驶概念车的先驱。
1957年，在内布拉斯加州一条高速公路上，美国一些研究人员，通过埋在地下的探测器检测道路上的障碍物。
1960年，英国研究人员利用埋在道路中的信号电缆网络为提供给汽车道路信息，控制汽车自动转向、加速或制动等。要实现自主导航、运动跟踪、障碍物检测和规避等一系列功能，要求移动机器人必须保证能够获得随着时间的推移获取自身位置的改变。
%为解决移动机器人定位问题科研人员尝试了不同的传感器、方案
%不过值得庆幸的是， 每一个逐梦者的挫折都让无人车的前进之路日渐清晰——这是一条智能进化之路，也是人类社会的变革之路。
%无人车利用各种车载传感器来感知车辆周围环境，根据感知所获得的环境 与车辆状态信息，控制车辆安全、可靠地在各种道路上行驶。无人车作为现代 化战争的新概念、
%汽车技术发展的新方向和科学研究的综合验证平台，一直倍受国防事业、汽车工业和高校与科研机构的关注

近年来为了保障机器人能够获得随着时间的推移其位置的改变，研究人员和工程师们开发了各种用于移动机器人定位的传感器、技术和系统，如轮式里程计IMU、惯性导航系统（INS）、激光或超声波测距法、全球定位系统(GPS)和视觉里程计（VO）等。然而，每种方案都有自己的不可避免的弱点：IMU是最简单的位置估计技术，但由于车轮滑移，它存在位置漂移问题\cite{2005Visual}；INS也极易产生漂移，而高精度的INS价格昂贵，对于商业用途来说是不可行的解决方案；
激光测距仪以激光器作为光源，由光电元件以一定的工作频率向目标物体发射并接收目标反射的激光束，由计时器来测定激光束从发射出去到接收的时间差，从而计算出观测者到目标物体的距离，由于不受主动光源的影响，激光测距仪可以在黑暗环境中正常工作。超声波测距原理与激光测距原理相似，不过发射装置发出的是超声波而不是激光；GPS是最常见的定位解决方案，因为它可以提供绝对的位置而不会积累误差，但它只在天空视野清晰的地方有效，它不能在室内、深海、密闭空间等环境中使用\cite{2011Combined}。商用GPS所估计位置的误差也较大，通常其误差是在米级。这种误差被认为对于要求精度以厘米为单位的精确应用来说太大，例如室内服务机器人小面积精准定位和自主停车等。差分式全球定位系统和实时运动式全球定位系统可以提供厘米级精度的位置，但这些技术成本较高价格昂贵。GPS是一项已经深深的融入到了我们老百姓的日常生活中的定位方式，包括车载型、通讯型、便携型、船载型、指挥型等多种用户终端，具体如每日必不可少的车辆导航、手机定位等。%其最早应用可以追溯到冷战时期，真正达到民用是在1993年6月26日，美国空军将第24颗Navstar卫星送入轨道，完成了由24颗卫星组成的网络，
GPS接收器的可以保证以较低的价格就可以立即获取我们在地球上所处的10米级的位置，包括纬度，经度和海拔。根据美国政府有关全球定位系统(GPS)\footnote{https://www.gps.gov/chinese.php}的官方信息，截至2020年5月，有29颗可运行卫星。
%卫星每天以20,200公里（12,550英里）的速度绕地球飞行两次。美国空军监视和管理该系统，并承诺在95％的时间中至少拥有24颗卫星，其中21颗工作卫星，3颗备用卫星。24颗卫星
%运行在6个轨道平面上，运行周期为12个小时。保证在任一时刻、任一地点高度角15度以上都能够观测到4颗以上的卫星。
卫星导航系统虽然最早是美国发明并投入使用的，但其他国家也开发了自己的卫星导航系统，比如我们国家自主研发的北斗卫星导航系统、俄罗斯的格洛纳斯系统等。%、欧盟的伽利略定位系统。除伽利略系统外，其余系统都已投入运营。

本文研究重点之一是增量式单目视觉里程计，其是解决机器人定位的最快捷方便且低成本的方式之一。
\section{国内外研究现状介绍}
\subsection{单目视觉绝对定位研究现状}
在绝对定位方中，机器人对世界的认知须以地图的形式存储，进而与当前的观察结果进行对比，通过GPS等tag完成定位。通过对图像进行特征提取并检索以完成匹配。然而用一个稳定鲁棒的特征来表示一个处于变化的动态场景是一个很大的挑战，如图\ref{fig:Environments}所示。

\subsubsection{图像特征提取相关工作}
根据运算过程中采用的图片帧数，单目视觉定位的方法可分为两类：基于单帧图像的定位方法，基于两帧或多帧的定位方法。基于单帧图像的定位方法包括基于特征点的定位（Perspective-n-Point）、基于直线或平面特征的定位，其关键点在于快速准确地实现投影图像与模板之间的特征匹配。基于两帧或多帧图像的定位方法的关键在于实现多帧投影图像之间的对应特征元素匹配，如SLAM。

%根据图像分类，给出了最先进的手工制作特征和非手工制作特征的总体比较。

%对于手动提取特征，各种各样的最先进的算法被认为是\cite{anni2017handcraft}：局部三元模式、局部相位量化、完成的局部二进制模式、旋转不变共现局部二进制模式、旋转局部二进制模式图像、全局旋转不变多尺度共现局部二进制模式和其他几种算法。

%\subsubsection{局部特征手动提取}
从相机采集的视觉数据中提取特征，即图像特征提取是机器人位置识别的一个基本问题，其中图像特征提取的方式是影响定位性能的关键。局部特征（local features），仍是近年来研究的一个热点。局部特征指一些能够稳定出现并且具有良好的可区分性的稳定特征点。局部特征数量丰富，特征间相关度小，不容易受到部分遮挡、光照等噪声的干扰，因为不会因为部分特征的消失而影响其他特征的检测和匹配，这样如果我们用这些稳定出现的点来代替整幅图像，可以大大降低图像原有携带的大量信息，起到减少计算量的作用，且在物体不完全受到遮挡的情况下，一些局部特征依然稳定存在，以代表这个物体（甚至这幅图像），方便进一步分析与运算。

因此，大量的计算机视觉研究集中在如何手动提取特征上，以发现和描述从图像中提取的特征，如SURF\cite{SURF2006surf}、ORB\cite{rublee2011orb}、BRIEF\cite{calonder2010brief}、SIFT\cite{lowe1999object}、Harris\cite{harris1988combined}、SIFT\cite{lowe1999object}和HOG\cite{Dalal2005Histograms}。

%我们可以看下面这个图，左边一列是完整图像，中间一列是一些角点（就是接下来我们要讲的局部特征），右边一列则是除去角点以外的线段。不知道你会不会也觉得你看中间一列的时候能更敏感地把他们想象成左边一列的原始物品呢？

%\subsubsection{全局特征手动提取}
如果用户对整个图像的整体感兴趣，而不是前景本身感兴趣的话，全局特征用来描述总是比较合适的。但是无法分辨出前景和背景却是全局特征本身就有的劣势，特别是在我们关注的对象受到遮挡等影响的时候，全局特征很有可能就被破坏掉了。

非手工特征采用三种方法：基于卷积神经网络(CNN)的深度转移学习特征、主成分分析网络(PCAN)和紧凑的二进制描述符(CBD)\cite{nanni2017handcraft}。 在我们以前的工作中，我们还尝试用IPCA来减少特征维数\cite{zhang2017Dynamic}，我们的结果证明了33维深度特征可以在匹配矩阵中以高精度识别。
%\subsubsection{深度学习特征}
最近，非手工制作的特征能够通过深度卷积神经网络(DCNN)从数百万标记图像中自动学习鉴别特征，这在计算机视觉和机器学习社区的几乎所有重要任务中都取得了最先进的性能\cite{Radford2016Unsuperved}\cite{Chen20143D}\cite{Krizhevsky2012ImageNet}\cite{Simonyan2014Very}\cite{Szegedy2015Going}\cite{He2015Deep}。%一个典型的DCNN由许多卷积层和池化层组成，然后是全连接层\cite{Krizhevsky2012ImageNet}。



最近的文献提出了多种方法来解决这个领域的挑战\cite{milford2012seqslam}\cite{corke2013dealing}\cite{neubert2015superpixel}\cite{mcmanus2015learning}\cite{naseer2014robust}\cite{churchill2012practice}\cite{lowry2014transforming}。众所周知，在2012年的AlexNet大规模视觉识别挑战赛（ILSVRC）上一个新的网络模型CNN获得了令人难以置信的准确率\cite{krizhevsky2012imagenet}。 

文章\cite{donahue2014decaf}\cite{girshick2014rich}\cite{krizhevsky2012imagenet}\cite{sermanet2013overfeat}研究表明，来自神经网络的网络特征优于传统的手动特征 Cite{sharif2014cnn,ORB2011orb,surf2006surf,lowe2004distinctive}。
该网络由5个卷积层，以及3个全连接层和soft-max层组成，它在120万张有标签的图像上进行了预训练。根据从AlexNet中提取的特征对图像进行分类。每个单独层的输出可以作为一个全局的图像描述符。我们还可以根据这些特征对图像进行匹配，然后定位机器人。
\cite{donahue2014decaf}表示，来自CNN中层的特征可以更有效地消除数据集的偏差。\cite{sunderhauf2015performance}比较了不同层特征的性能。他们的结果表明，来自ConvNet层次结构中的中间层的特征对一天中的时间、季节或天气条件引起的外观变化表现出鲁棒性。Conv3层的特征在面对极端的外观变化时表现得相当好。表1列出了AlexNet网络中不同层的向量尺寸。\cite{sunderhauf2015performance}证明了Conv3层的特征在外貌变化方面的表现相当好。此外，fc6和fc7在视角变化方面优于其余层。但是，当外观变化时，fc6和fc7完全失效。Conv3的维度为64896，即一幅图像显示为64896维度的矢量。在线定位将持续接收来自摄像头的图像。毋庸置疑，大量高维度向量数学运算增加了运算时间。它在120万张有标签的图像上进行了预训练。根据从AlexNet中提取的特征对图像进行分类。每个单独层的输出可以作为一个全局的图像描述符。我们还可以根据这些特征对图像进行匹配，然后定位机器人。

%DCNN中包含的不同特征最初是以浮动格式返回的。
为了方便后续的二值化，\cite{arroyo2016fusion}将这些特征投向一个规范化的8位整数格式。然后利用汉明距离对所有二进制特征进行匹配，计算出一个匹配矩阵。他们的研究结果表明，对特征进行压缩可以极大程度上降低其描述符的冗余度，而精度只降低了约2\%。此外，他们对特征的二值化允许使用汉明距离，这也代表了位置匹配的加速。在减少特征集的情况下，改进了地点识别。

\subsubsection{图像特征匹配}
机器人视觉图像匹配是指机器人定位领域的场所识别，是继特征提取之后的另一个挑战。毋庸置疑，在绝对定位方中，机器人对世界的认知须以地图的形式存储，进而与当前的观察结果进行对比，通过GPS等tag完成定位。文章\cite{lowry2016visual}指出，根据视觉传感器的不同，以及识别场景种类的不同，地图框架也有所不同。可以分为纯图像检索、拓扑地图和拓扑-度量地图。纯图像检索只存储环境中每个地方的外观信息，没有相关的位置信息，例如FAB-MAP中使用的Chow-Liu树结构\cite{cummins2008fab}。FAB-MAP\cite{cummins2008fab}描述了一种概率方法来解决匹配图像和地图增强的问题。他们使用了基于向量的描述符，如与bag-of-words联合的SURF特征。FAB-MAP\cite{cummins2008fab}描述了一种概率方法来解决匹配图像和地图增强的问题。他们使用了基于向量的描述符，如SURF与Bag-of-Words联合使用。他们通过构建一个Chow-Liu树结构\cite{chow1968approximating}来捕捉视觉词的共现统计，学习了一个图像深度网络特征的生成模型。Chow-Liu树由节点和边组成。变量之间的相互信息关联度由节点之间边的粗细来显示，图中的每一个节点对应一个由传感器数据转换而来的词袋，图中的每一个节点对应一个由传感器数据转换而来的词袋，变量之间的相互信息通过树的边的粗细来显示。图中的每一个节点对应一个由输入感官数据转换而来的词袋表示。在具有挑战性的户外环境中，FAB-MAP能够成功地检测到了大部分的闭环场景。但\cite{naseer2014robust}的结果显示，在跨季节的数据集中，OpenFABMAP2只找到了少数正确的匹配，原因是，传统手工特征描述符是不可重复的。 论文cite{naseer2014robust}将图像匹配制定为数据关联图中的最小成本流问题，以有效利用序列信息。他们通过最小成本流定位车辆。他们的方法即使在高度变化的动态场景也表现良好。SeqSLAM \cite{milford2012seqslam}将图像识别问题构思为在局部邻域内寻找所有与当前图像最佳匹配的模板。这很容易实现。然而，\cite{milford2012seqslam}很容易受到机器人速度的影响。这种影响限制了机器人进行长时间自我定位。%\cite{Schindler2007City}证明，如果使用每张图像中信息量最大的特征，地方识别性能就会提高。


\subsection{增量式定位--单目视觉里程计研究现状}
里程计，英文是"odmetry"，该词源于两个希腊词hodos（意为 "旅程"或者"旅行"）和metron（意为 "测量"）\cite{2005Visual}。这一推导与估计机器人姿势（平移和方向）随时间的变化有关。移动机器人使用来自运动传感器的数据来估计它们相对于初始位置的位置；这个过程被称为里程测量。里程计是使用来自运动传感器的数据来估算其位置随着时间变化的一种定位技术，一些腿式或轮式机器人在机器人定位技术中使用它来估计其相对于起始点的位置。由于对速度测量值进行了时间积分，因此该方法对误差很敏感，可以给出位置估计值。在大多数情况下，需要快速而准确的数据收集，仪器标定校准和快速处理才能有效使用里程计。

视觉里程计，即VO（visual odometry），是一种视觉定位技术，视觉里程计(VO）是一种仅仅通过从连接到机器人的单个或多个摄像头获取的图像序列来实现机器人定位\cite{2008Monocular}移动机器人的增量式定位过程。这些图像包含足够多的有意义的信息（颜色、纹理、形状等），以估计相机在静态环境中的运动\cite{Rone2013Mapping}。增量定位是连续观察机器人姿态的变化，通过累积运动计算机器人当前姿态。VO是未探索环境中机器人自定位和自主导航的基本模块，因为它不依赖于预先构建的地图\cite{liu2012finding}\cite{salarian2018improved}。单目视觉里程计(MVO)由于配备了最便宜、使用最广泛的传感器，也是最方便校准的传感器，因此引起了机器人界的广泛研究兴趣。同时，它不受固定基线长度的限制，可以在不同的场景中广泛使用。然而，当单目相机将三维(3D)世界投影到二维(2D)平面空间时，它失去了物体的深度信息和绝对尺度。因此，MVO只能获得相对的，而不是机器人运动的绝对距离。这种尺度模糊可以积累尺度误差，称为尺度漂移。尺度模糊和尺度漂移统称为尺度问题。VO已经研究了30多年，最初是由NASA的火星探索项目推动的\cite{scaramuzza2012visual}。与IMU和雷达等其他增量定位系统相比，单目视觉里程法(MVO)的优点是显而易见的。它配备了最便宜和使用最广泛的传感器，也是最方便的校准。 因此，MVO是机器人界的一个活跃的研究领域。

尺度问题严重限制了MVO\cite{scaramuzza2011vo}的精度，相应的解可分为相对尺度修正和绝对尺度估计。前者主要包括光束法平差（BA）\cite{triggs1999bundle}和环路闭合(LC)检测。 虽然它们确实在限制尺度问题上起作用，但它们无法检索机器人的度量信息。 对于MVO系统，只有借助先验知识，引用绝对量度信息才能解决尺度问题。流行的绝对量度信息参考包括基线距离（双目摄像机)、摄像机高度(道路模型）和从其他传感器或离线训练中获得的像素深度等\cite{Costante2015Exploring}。 其中，挂载相机绝对高度是常用的，因为它既不需要其他传感器的帮助，也不需要离线训练，是最方便测量和校准的。此外，在车辆运行过程中，摄像机绝对高度保持稳定。

在已安装摄像机高度的先验知识下，MVO的精度取决于相对尺度下的道路几何估计。 当使用相机高度作为尺度恢复的绝对参考时，有必要获得道路的几何模型。其中包含估计的摄像机高度。许多方法\cite{kitt2011mono}\cite{Song2015MoncularScale}\cite{zhou2016reliable}根据先验知识选择一个感兴趣的区域(ROI)，或自动检测的确定固定区域\cite{chen2007automatically}作为道路区域。然而，基于ROI的方法有两个缺点。首先，不能保证所选区域始终为路面。此外，图像信息不能得到充分利用。道路检测解决方案更合理，因为它从整个图像中提取特征。此外，由于深度学习方法在多个领域取得超越性性能，\cite{hoiem2007recovering}提出的分割方法和训练分类器也用于道路检测。但这种方法的计算成本较高且对不熟悉的情况不够鲁棒。此外，所有以前基于分类器的方法都集中在道路的颜色信息上，虽然在深度学习的帮助下，基于道路颜色信息的视觉里程法可以得到很大的改进，此类方法对光照、阴影和材料等因素依然较敏感。因此，我们将颜色信息替换为结合道路几何约束的道路点选择。通过在线更新道路分类器使\cite{Lee2015MoncularScale}中的框架更加鲁棒。

虽然融合传感器测量系统目前在精度、鲁棒性和可靠性方面处于领先地位\cite{ye2019tightly}\cite{zhang2014loam}。单目视觉里程计(MVO)却有可能取代它们。MVO面临的许多挑战\cite{scaramuzza2011visual}主要存在于大规模、动态或无特征的环境中)。




\subsubsection{相对尺度校正}
光束法平差\cite{triggs1999bundle}和环路闭合检测相对尺度校正的两种重要方法。尺度校正被光束法平差描述为一个非线性最小二乘问题，以产生联合最优的三维结构和摄像机姿态估计。Mouragon \textit{et al}.\cite{mouragnon2006real}第一次在实时VO中利用光束法平差，其次是并行跟踪和映射(PTAM)，这是定向FAST和旋转BRIEF同时定位和映射(ORB-SLAM)的主要动机。然而，局部和全局光束法平差优化都
存在严重的累积尺度误差。环路闭合检测是一种在先前访问过的地点进行比对的技术，以修正产生的位移偏差；

\cite{nister2006scalable}提出了一种词袋(BoW)方法来表示关键帧。 基于快速外观的映射(FAB-MAP)\cite{cummins2008fab}是一种经典的位置识别方法，它与Chow-Liu树\cite{chow1968approximating}一起构造了BoW模型的视觉词汇表，以表达其特征相似性。 然而，在实际的交通场景中，环路很少出现，关键帧的选择严重影响了环路闭合检测的准确性\cite{piao2019real}。此外，在长距离驾驶中，光束法平差下的尺度漂移也变得严重\cite{mouragnon2006real}\cite{klein2007parallel}。

FAB-MAP\cite{cummins2008fab}是一种经典的位置识别方法。它不仅限于定位任务，而且可以判断一个新的观察是否来自地图上已经存在的地方。FAB-MAP与Chow-Liu树\cite{chow1968approximating}一起构造了BoW模型的视觉词汇表，以表达其特征相似度。该方法也应用于另一个优秀的工作ORB-SLAM2\cite{Mur2016ORB}。 该方法基于面向FAST和旋转的关键帧的BRIEF（ORB）描述符离线训练大量的BoW。当摄像机返回到以前的场景时，它将获得类似的BoW描述符，从而检测环路闭合。

\subsubsection{绝对尺度恢复}
绝对尺度恢复方法可以补偿以已知的度量信息作为参考的相对尺度校正的局限性，例如从深度学习中学到的安装相机高度和图像深度。
\paragraph{相机高度-固定方法}
摄像机高度约束方法的区别主要在于道路平面的检测和建模方法。许多方法\cite{kitt2011mono}，\cite{Song2015MoncularScale}，\cite{zhou2016reliable}假定ROI为道路。 
来自运动的单目大规模多核结构(MLM-SFM)方法\cite{Song2015MoncularScale}中，扩展了\cite{song2014robust}和\cite{Song2013Parallel}，假设图像的下三分
之一的中五分之一为ROI。MLM-SFM提出了一种数据驱动机制，将多个线索组合在一个框架中，该框架反映了它们的每帧相对机密性，这显示了很有前途的性能。然而，当所选区域被
汽车或其他东西遮挡时，基于RIO的方法无法工作，如KITTI数据集的序列07中所发生的那样，因此MLM-SFM不能像我们所预期的那样在它该环境运行。同时图像信息可能无法充分
利用，因为ROI只是图像的一小部分。第二个缺点可以用双向或全向相机\cite{gutierrez2012full}，\cite{scaramuzza2008appearance}，\cite{scaramuzza2006flexible}弥补。但是考虑到传感器的成本及使用便捷性，、
普通单目相机更具有研究价值。

道路平面估计方法将判断哪些点属于道路进行帧对帧运动估计，会进行特征点筛选并充分利用整张图像信息，因此更合理。 这些方法可根据其特征点大小分为稀疏\cite{engel2017direct}，半稠密\cite{Engel2014LSD}\cite{forster2014svo}，稠密\cite{newcombe2011dtam}。 描述符也可能是由一些视觉过程\cite{chen2012tag}增强或从卷积神经网络（CNNs）中提取\cite{lin2017hnip}\cite{chadha2017voronoi}。除了特定特征点的方法外，多种描述符相结合的方法也很流行。 该方法将来自稠密和稀疏匹配点的线索结合起来，并使用分类器基于各种特征检测尺度异常值，这确实提高了对各种地面结构的鲁棒性。然而，它依赖于稠密的特性，如果没有GPU的帮助，它就不能很容易地在移动嵌入式系统上实现。 我们的方法在没有稠密特征的任何帮助下取得了有竞争力的结果。

在道路点检测后，一些方法\cite{Gall2015}选择利用三角稀疏地面点计算高度，然后估计绝对尺度。传统上，采用三点RANSAC\cite{choi1997performance}来实现鲁棒平面拟合。
在非传统方法中，\cite{pereira2018novel}用逆向选择代替RANSAC，\cite{Geiger2011IV}用一种快速匹配方法对一组选定的点进行三角剖分，该方法称为有效的大规模立体声\cite{geiger2010efficient}。

\paragraph{基于图像深度的方法}
最近，MVO有一个与深度学习相结合的流行趋势，其中包括从图像与CNN估计的深度。对于MVO的训练数据，\cite{rukhovich2019estimation}中的结果表明，从合成训练输入中获得的尺度估计精度与从实际数据中获得的估计精度相似。在\cite{saxena2006learning}中，使用深度CNN与CRF相结合的方法来估计在小规模和大规模环境中拍摄的单目图像的深度。Luo \textit{et al.}将在线自适应深度与直接单目SLAM相结合\cite{luo2018real}，提高了不同场景的深度预测精度。它有望解决两个核心挑战：地图完整性低和尺度模糊。然而，单帧图像的深度估计\cite{karsch2014depth,ranftl2016dense,yang2019bayesian,eigen2014depth,saxena2006learning}比来自连续帧的深度估计更复杂\cite{yang2019bayesian}。\cite{yin2017scale}提出了一种新颖的监督系统从估计的深度图计算平移的尺度，将条件随机场与CNN网络相结合以优化深度图，这是通过考虑两个连续图像和运动约束来改进的。

深度预测的准确性对单目SLAM中的特征跟踪误差有巨大影响。CNN-SLAM\cite{tateno2017cnn}扩展了大规模直接SLAM(LSD-SLAM)\cite{engel2014lsd}，通过部署深度神经网络的预测深度图来产生密集的3D地图。它在室内数据集\cite{Sturm2012A,2014A}中取得了很好的效果，但在多个关键帧重叠时，预测的深度图无法优化，从而使得重建和映射的精度降低。DVSO\cite{yang2018deep}使用与\cite{godard2017unsupervised}类似的虚拟立体视图，将深度预测纳入几何单目测绘流水线。Luo等人\cite{luo2018real}将在线适应深度与直接单眼SLAM相结合，提高不同场景的深度预测精度。这些方法都有希望解决地图完整性低和比例尺模糊性这两个核心难题。

从连续帧中进行深度估计比从单幅图像中进行深度估计更容易\cite{yang2019bayesian,eigen2015predicting}。\cite{Costante2015Exploring}的方法从连续图像中提取密集的光流，并训练一个基于深度CNN的估计器来进行自运动估计。本研究中的新型监督系统通过考虑两幅连续图像和运动约束，从估计的深度图中计算出转换的尺度，并对其进行了改进。他们的网络是通过并发CNN和条件随机场来构建的，以完善深度图。除了估计单视角深度，\cite{zhan2020visual}还尝试估计双视角光流作为另一个中间输出。最近，Xue等人\cite{xue2020toward}提出了一种利用密集法线进行道路检测的新方法，在几何约束方面与我们的方法类似。这些基于端到端深度学习的SLAM系统已经取得了令人印象深刻的性能，然而，CNN的深度预测不准确会严重导致单目SLAM中的特征跟踪误差，且它们都需要进行离线训练，增加了时间成本与计算成本。此外，也不能保证它们能泛化到新的环境中。我们的系统不仅可以在新的环境中工作，而且可以用低成本的硬件达到较好的性能。





视觉里程计的实现方法？按照传感器数目的不同可以分为：多目相机、双目相机、单目相机，其中相机的类型包括普通相机、鱼眼相机和全景相机等。双目里程计与单目里程计是视觉里程计的研究热点，双目VO的优势在于，对机器人的运动轨迹估计更加精确，且具备明确的距离单位。而在单目VO中，如果没有已知量度大小作为参考我们只能知道物体在x/y方向上移动了1个或多个单位，却不知道具体的单位大小。但是，单目视觉里程计比双目里程计更有研究价值，原因如下：单目相机比双目相机更加便于标定校准、当物体距离机器人很远的，双目系统又退化为单目系统。而且当机器人形体很小时，对单目相机被占据更小空间，即使安装了双目相机，但是因为两个相机距离及其近，又可被近似为单目系统。

视觉里程计的主要工作就是计算从图像$I_t$到图像$I_{t+1}$位置变换$T_k$ ，然后集成所有的姿态变换恢复出相机相对于初始位置坐标$\mathbf{P}_0$的位姿$\mathbf{P}_t$，这意味着VO是一种增量式轨迹重建方法。

\section{本文内容与贡献}
本文提出了哪几种方案，主要贡献是：
绝对定位之单目深度特征提取与压缩
增量定位之单目视觉里程计的绝对尺度运算
增量定位之单目视觉里程计求解新框架简化
\begin{enumerate}
	\item 基于固定相机高度，基于路面几何约束的单目视觉里程计尺度恢复。
	\item 基于单目模型的单目尺度计算：从手动建模到自主学习
	\item 传统视觉位姿估计与深度学习尺度恢复的结合
	\item 视觉定位及其与视觉里程计的结合
\end{enumerate}

\section{全文架构}
